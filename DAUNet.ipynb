{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80666d1-6740-41f6-ace9-2e6858cf8f98",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de1380-2891-486f-88f4-91b4606561a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from data_generator import DataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from train_val_epoch import train_epoch, validation_epoch\n",
    "from metrics import plot_feature_space, meanf1_iou, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7b266-cd8b-4982-b569-25461a269675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TensorFlow {}; Keras {}'.format(tf.__version__, keras.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0b4c5-5c1d-4ddf-b48b-a3b3616aaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd8cc8-1ded-4316-8f9b-13d1c0c92d4b",
   "metadata": {},
   "source": [
    "# Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfa2ec-f1fb-4cf4-a2fe-ecc6cde71637",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./west/\"\n",
    "TAR_BASE_PATH = \"./east/\"\n",
    "NUM_CLASSES = 4\n",
    "im_height, im_width = 256, 256\n",
    "test_ratio = 0.1\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e7e01-f506-416b-b24a-15ce54ffa4c3",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518e356-189b-455a-a2f4-1ef9a90414b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrac name of files\n",
    "files = os.listdir(BASE_PATH)\n",
    "tar_files = os.listdir(TAR_BASE_PATH)\n",
    "files = [BASE_PATH + f for f in files]\n",
    "tar_files = [TAR_BASE_PATH + f for f in tar_files]\n",
    "print(\"###SRC FILES###\")\n",
    "print(BASE_PATH)\n",
    "print(len(files))\n",
    "print(\"###TAR FILES###\")\n",
    "print(TAR_BASE_PATH)\n",
    "print(len(tar_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5289a9d-9b22-4cb1-b0ce-052e8e140b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "random.shuffle(files)\n",
    "random.shuffle(tar_files)\n",
    "test_size = int(len(files) * test_ratio)\n",
    "\n",
    "test_files = files[:test_size]\n",
    "non_test_files = files[test_size:]\n",
    "\n",
    "val_size = int(len(non_test_files) * test_ratio)\n",
    "val_files = non_test_files[:val_size]\n",
    "train_files = non_test_files[val_size:]\n",
    "\n",
    "print(\"Train size:\", len(train_files))\n",
    "print(\"Validation size:\", len(val_files))\n",
    "print(\"Test size:\", len(test_files))\n",
    "print(train_files[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b76b17-0221-4644-a2ff-dee45d41f72a",
   "metadata": {},
   "source": [
    "# importing models from keras_unet_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c069d-421c-411e-8c89-55d42ae3d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ba1f8-3bbf-457a-aaf3-56ffe59fb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model, model = models.unet_2d((256, 256, 12), [16, 32, 32, 64], n_labels=NUM_CLASSES,\n",
    "                      stack_num_down=2, stack_num_up=1,\n",
    "                      activation='GELU', output_activation='Softmax', \n",
    "                      batch_norm=True, pool='max', unpool='nearest', name='unet',\n",
    "                      is_domain_adaptation=True, da_type='conv2d', da_kernels=[64, 32, 32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cc9a9-1256-4fc0-8b3d-f6800bcbbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum, minimum = 14.733826, -49.208305\n",
    "\n",
    "train_gen = DataGenerator(image_paths=train_files, batch_size=BATCH_SIZE,  augment=True, \n",
    "                          shuffle=True, normalize=True, maximum=maximum, minimum=minimum)\n",
    "val_gen = DataGenerator(image_paths=val_files, batch_size=BATCH_SIZE, augment=False, shuffle=False,\n",
    "                        normalize=True, maximum=maximum, minimum=minimum)\n",
    "test_gen = DataGenerator(image_paths=test_files, batch_size=BATCH_SIZE, augment=False, shuffle=False,\n",
    "                         normalize=True, maximum=maximum, minimum=minimum)\n",
    "\n",
    "tar_gen = DataGenerator(image_paths=tar_files, batch_size=BATCH_SIZE, augment=False, shuffle=False,\n",
    "                         normalize=True, maximum=maximum, minimum=minimum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92498e48-6a25-4ad6-b655-4874684634ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import weightedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ae2f8-e743-4edf-9641-13ae1d07c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "other = 17749814 + 17766350 + 22149798\n",
    "corn = 204516 + 172453 + 235173\n",
    "cotton = 88734 + 26780 + 1677\n",
    "rice = 10122026 + 6884977 + 7128782\n",
    "total = other + corn + cotton + rice \n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_other = (1 / other) * (total / 2.0)\n",
    "weight_for_corn = (1 / corn) * (total / 2.0)\n",
    "weight_for_cotton = (1 / cotton) * (total / 2.0)\n",
    "weight_for_rice = (1 / rice) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_other, 1: weight_for_corn, 2: weight_for_cotton, 3: weight_for_rice}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_other))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_corn))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_cotton))\n",
    "print('Weight for class 3: {:.2f}'.format(weight_for_rice))\n",
    "\n",
    "weights = [weight_for_other, weight_for_corn, weight_for_cotton, weight_for_rice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36d5d2-4d0a-4e11-ac7c-c00744aa8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=weightedLoss(keras.losses.categorical_crossentropy, weights),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1069a-b6a5-4c25-9859-821137f64564",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf5fdf-a0b4-4295-af0a-81ed6d811438",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 8\n",
    "iterations = 3\n",
    "\n",
    "\n",
    "src_seg_acc_train_list = list()\n",
    "src_seg_acc_test_list = list()\n",
    "tar_seg_acc_test_list = list()\n",
    "src_dom_acc_train_list = list()\n",
    "src_dom_acc_test_list = list()\n",
    "tar_dom_acc_train_list = list()\n",
    "tar_dom_acc_test_list = list()\n",
    "src_dom_loss_train_list = list()\n",
    "tar_dom_loss_train_list = list()\n",
    "\n",
    "losses = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4677b1-aa57-4371-9f27-9bb2dc443d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define writer to write histories\n",
    "writing_path = \"mylogs\" + str(0)\n",
    "writer = tf.summary.create_file_writer(writing_path)\n",
    "\n",
    "with writer.as_default():\n",
    "    src_seg_acc_train_list_ = list()\n",
    "    src_seg_acc_test_list_ = list()\n",
    "    tar_seg_acc_test_list_ = list()\n",
    "    src_dom_acc_train_list_ = list()\n",
    "    src_dom_acc_test_list_ = list()\n",
    "    tar_dom_acc_train_list_ = list()\n",
    "    tar_dom_acc_test_list_ = list()\n",
    "    src_dom_loss_train_list_ = list()\n",
    "    tar_dom_loss_train_list_ = list()\n",
    "    src_seg_loss_train_list_ = list()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # train model in one epoch\n",
    "        (seg_loss, \n",
    "        src_dom_loss, \n",
    "        tar_dom_loss, \n",
    "        seg_train_acc, \n",
    "        src_dom_acc, \n",
    "        tar_dom_acc) = train_epoch(model, d_model, BATCH_SIZE, train_gen, tar_gen)\n",
    "\n",
    "        # keep results in lists\n",
    "        src_seg_acc_train_list_.append(seg_train_acc)\n",
    "        src_seg_loss_train_list_.append(seg_loss)\n",
    "        src_dom_acc_train_list_.append(src_dom_acc)\n",
    "        src_dom_loss_train_list_.append(src_dom_loss)\n",
    "        tar_dom_acc_train_list_.append(tar_dom_acc)\n",
    "        tar_dom_loss_train_list_.append(tar_dom_loss)\n",
    "\n",
    "        # write the results in tf.summary\n",
    "        tf.summary.scalar('seg_loss_train', seg_loss, step=epoch)\n",
    "        tf.summary.scalar('src_dom_loss_train', src_dom_loss, step=epoch)\n",
    "        tf.summary.scalar('tar_dom_loss_train', tar_dom_loss, step=epoch)\n",
    "        tf.summary.scalar('seg_acc_train', seg_train_acc, step=epoch)\n",
    "        tf.summary.scalar('src_dom_acc_train', src_dom_acc, step=epoch)\n",
    "        tf.summary.scalar('tar_dom_acc_train', tar_dom_acc, step=epoch)\n",
    "\n",
    "        print('Train: Epoch %s: Seg Loss: %.4f, Src Dom Loss: %.4f, Tar Dom Loss: %.4f, Seg Acc: %.4f, Src Dom Acc: %.4f, Tar Dom Acc: %.4f' % \n",
    "        (epoch, seg_loss, src_dom_loss, tar_dom_loss, seg_train_acc, src_dom_acc, tar_dom_acc))\n",
    "\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            (src_seg_loss, \n",
    "            tar_seg_loss,\n",
    "            src_dom_loss, \n",
    "            tar_dom_loss, \n",
    "            src_seg_acc, \n",
    "            tar_seg_acc,\n",
    "            src_dom_acc, \n",
    "            tar_dom_acc) = validation_epoch(model, d_model, BATCH_SIZE, val_gen, tar_gen)\n",
    "\n",
    "            src_seg_acc_test_list_.append(src_seg_acc)\n",
    "            tar_seg_acc_test_list_.append(tar_seg_acc)\n",
    "            src_dom_acc_test_list_.append(src_dom_acc)\n",
    "            tar_dom_acc_test_list_.append(tar_dom_acc)\n",
    "            tf.summary.scalar('seg_loss_test_src', src_seg_loss, step=epoch)\n",
    "            tf.summary.scalar('seg_loss_test_tar', tar_seg_loss, step=epoch)\n",
    "            tf.summary.scalar('src_dom_loss_test', src_dom_loss, step=epoch)\n",
    "            tf.summary.scalar('tar_dom_loss_test', tar_dom_loss, step=epoch)\n",
    "            tf.summary.scalar('src_seg_acc', src_seg_acc, step=epoch)\n",
    "            tf.summary.scalar('tar_seg_scc', tar_seg_acc, step=epoch)\n",
    "            tf.summary.scalar('src_dom_acc', src_dom_acc, step=epoch)\n",
    "            tf.summary.scalar('tar_dom_acc', tar_dom_acc, step=epoch)\n",
    "\n",
    "            print('Test: Epoch %s: Src Seg Loss: %.4f, Tar Seg Loss: %.4f, Src Dom Loss: %.4f, Tar Dom Loss: %.4f, Src Seg Acc: %.4f, Tar Seg Acc: %.4f, Src Dom Acc: %.4f, Tar Dom Acc: %.4f' % \n",
    "            (epoch, src_seg_loss, tar_seg_loss, src_dom_loss, tar_dom_loss, src_seg_acc, tar_seg_acc, src_dom_acc, tar_dom_acc))\n",
    "            model.save_weights(f'DAUNet1/{epoch}')\n",
    "            \n",
    "\n",
    "    src_seg_acc_train_list.append(src_seg_acc_train_list_)\n",
    "    src_seg_acc_test_list.append(src_seg_acc_test_list_)\n",
    "    tar_seg_acc_test_list.append(tar_seg_acc_test_list_)\n",
    "    src_dom_acc_train_list.append(src_dom_acc_train_list_)\n",
    "    src_dom_acc_test_list.append(src_dom_acc_test_list_)\n",
    "    tar_dom_acc_train_list.append(tar_dom_acc_train_list_)\n",
    "    tar_dom_acc_test_list.append(tar_dom_acc_test_list_)\n",
    "    src_dom_loss_train_list.append(src_dom_loss_train_list_)\n",
    "    tar_dom_loss_train_list.append(tar_dom_loss_train_list_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568cdf6",
   "metadata": {},
   "source": [
    "# Load model with best weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d4403",
   "metadata": {},
   "source": [
    "model.load_weights(best_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e8c54",
   "metadata": {},
   "source": [
    "# Plot Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, tar_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f7f37",
   "metadata": {},
   "source": [
    "# Calculate meanf1 and IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf1_iou(model, train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf1_iou(model, val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf1_iou(model, test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanf1_iou(model, tar_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8220e",
   "metadata": {},
   "source": [
    "# Plot bottleneck feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8885919",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_space(model, files, tar_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
