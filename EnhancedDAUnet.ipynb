{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBAZilGb5xus"
      },
      "source": [
        "first notebook lamda = 4 and second is lambda = 2\n",
        "this is 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDvwxxIYbts"
      },
      "source": [
        "# Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUBC24WUYlqF"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import warnings\n",
        "import os\n",
        "import random\n",
        "import datetime\n",
        "from tensorflow.python.ops.gen_array_ops import deep_copy\n",
        "\n",
        "from losses import IgnoreCaseWeightedLoss\n",
        "from data_generator import DataGenerator\n",
        "from metrics import accuracy\n",
        "from train_val_epoch import ignore_case_train_epoch, ignore_case_validation_epoch\n",
        "from metrics import plot_confusion_matrix_for_ignore_case, meanf1_iou_for_ignore_case\n",
        "from metrics import plot_feature_space\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gtoFiPhYnG4",
        "outputId": "489f8a24-ef80-4f8c-d53e-bfd70ee1ac42"
      },
      "outputs": [],
      "source": [
        "print('TensorFlow {}; Keras {}'.format(tf.__version__, keras.__version__))\n",
        "print(tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh1hxZaVYqtO",
        "outputId": "df56aa8d-dfbe-4acc-aec2-86bd11671dee"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"... path to source dataset\"\n",
        "TAR_BASE_PATH = \"... path to target dataset\"\n",
        "test_ratio = 0.1\n",
        "\n",
        "files = os.listdir(BASE_PATH)\n",
        "tar_files = os.listdir(TAR_BASE_PATH)\n",
        "files = [BASE_PATH + f for f in files]\n",
        "tar_files = [TAR_BASE_PATH + f for f in tar_files]\n",
        "print(\"###SRC FILES###\")\n",
        "print(BASE_PATH)\n",
        "print(len(files))\n",
        "print(\"###TAR FILES###\")\n",
        "print(TAR_BASE_PATH)\n",
        "print(len(tar_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7ftZ_jvYz1G",
        "outputId": "ca608044-1635-4ec0-da24-8ea382064b55"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "random.shuffle(files)\n",
        "random.shuffle(tar_files)\n",
        "test_size = int(len(files) * test_ratio)\n",
        "\n",
        "test_files = files[:len(tar_files)]\n",
        "non_test_files = files[len(tar_files):]\n",
        "\n",
        "val_size = int(len(non_test_files) * test_ratio)\n",
        "val_files = non_test_files[:val_size]\n",
        "train_files = non_test_files[val_size:]\n",
        "\n",
        "print(\"Train size:\", len(train_files))\n",
        "print(\"Validation size:\", len(val_files))\n",
        "print(\"Test size:\", len(test_files))\n",
        "print(train_files[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdEqY7Z4Y0jr"
      },
      "outputs": [],
      "source": [
        "maximum, minimum = 14.733826, -49.551544\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_gen = DataGenerator(image_paths=train_files, batch_size=BATCH_SIZE,  augment=True, \n",
        "                          shuffle=True, normalize=True, maximum=maximum, minimum=minimum)\n",
        "val_gen = DataGenerator(image_paths=val_files, batch_size=BATCH_SIZE, augment=False, shuffle=False,\n",
        "                        normalize=True, maximum=maximum, minimum=minimum)\n",
        "test_gen = DataGenerator(image_paths=test_files, batch_size=BATCH_SIZE, augment=False, shuffle=True,\n",
        "                         normalize=True, maximum=maximum, minimum=minimum)\n",
        "\n",
        "tar_gen = DataGenerator(image_paths=tar_files, batch_size=BATCH_SIZE, augment=False, shuffle=True,\n",
        "                         normalize=True, maximum=maximum, minimum=minimum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7IFqqcZwQVg"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3PTNtzQvTK3",
        "outputId": "4fd072a2-5ba9-43d2-865d-76c1c7255908"
      },
      "outputs": [],
      "source": [
        "other = 36107543\n",
        "corn = 551043\n",
        "cotton = 106433\n",
        "rice = 23218346\n",
        "total = other + corn + cotton + rice \n",
        "\n",
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_other = (1 / other) * (total / 2.0)\n",
        "weight_for_corn = (1 / corn) * (total / 2.0)\n",
        "weight_for_cotton = (1 / cotton) * (total / 2.0)\n",
        "weight_for_rice = (1 / rice) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_other, 1: weight_for_corn, 2: weight_for_cotton, 3: weight_for_rice}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_other))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_corn))\n",
        "print('Weight for class 2: {:.2f}'.format(weight_for_cotton))\n",
        "print('Weight for class 3: {:.2f}'.format(weight_for_rice))\n",
        "print('Weight for class 4: {:.2f}'.format(0.0))\n",
        "\n",
        "weights = [weight_for_other, weight_for_corn, weight_for_cotton, weight_for_rice]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YilcuQlevYZG"
      },
      "outputs": [],
      "source": [
        "weights = [weight_for_other, weight_for_corn*1.5, weight_for_cotton*1.5, weight_for_rice]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilm7fdiolU0z"
      },
      "source": [
        "# Create domain adaptive UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDxHCSeslvOO"
      },
      "outputs": [],
      "source": [
        "import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7yNPumgh2YR"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 4\n",
        "\n",
        "d_model, model = models.unet_2d((256, 256, 12), [16, 32, 32, 64], n_labels=NUM_CLASSES,\n",
        "                      stack_num_down=2, stack_num_up=1,\n",
        "                      activation='GELU', output_activation='Softmax', \n",
        "                      batch_norm=True, pool='max', unpool='nearest', name='unet',\n",
        "                      is_domain_adaptation=True, da_type='conv2d', da_kernels=[32, 16])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFz-_Foml3pm",
        "outputId": "3b155a22-f0ca-4dba-82d0-0a713e83baa1"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_EdHKDnm-F1",
        "outputId": "a6633a4d-e355-4431-a4f8-79cc789fec25"
      },
      "outputs": [],
      "source": [
        "d_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPc8UBggnHdZ"
      },
      "outputs": [],
      "source": [
        "weighted_crossentropy = IgnoreCaseWeightedLoss(keras.losses.categorical_crossentropy, weights, 4)\n",
        "binary_crossentropy = keras.losses.binary_crossentropy\n",
        "binary_accuracy = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "lr_schedule1 = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "lr_schedule2 = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-4,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer1 = keras.optimizers.Adam(learning_rate=lr_schedule1)\n",
        "optimizer2 = keras.optimizers.Adam(learning_rate=lr_schedule2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOmwfV1H3ohY"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abT9iREJJLWJ"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufWznescJO2H"
      },
      "outputs": [],
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UnjLUmAJpbN",
        "outputId": "f5118ea9-af62-4dc0-c41e-4bd3ca973687"
      },
      "outputs": [],
      "source": [
        "# segmentation_loss_mean = keras.metrics.Mean()\n",
        "# segmentation_accuracy_mean = keras.metrics.Mean()\n",
        "# domain_loss_mean = keras.metrics.Mean()\n",
        "# domain_accuracy_mean = keras.metrics.Mean()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # train model in one epoch\n",
        "    (segmentation_loss,\n",
        "    segmentation_accuracy,\n",
        "    domain_loss,\n",
        "    domain_accuracy) = ignore_case_train_epoch(model, \n",
        "                            d_model, \n",
        "                            BATCH_SIZE, \n",
        "                            train_gen, \n",
        "                            tar_gen,\n",
        "                            weighted_crossentropy,\n",
        "                            binary_crossentropy,\n",
        "                            optimizer1,\n",
        "                            optimizer2,\n",
        "                            accuracy,\n",
        "                            binary_accuracy\n",
        "                            )\n",
        "\n",
        "    # write the results in tf.summary\n",
        "    with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('train segmentation loss', segmentation_loss, step=epoch)\n",
        "        tf.summary.scalar('train segmentation accuracy', segmentation_accuracy, step=epoch)\n",
        "        tf.summary.scalar('train domain loss', domain_loss, step=epoch)\n",
        "        tf.summary.scalar('train domain accuracy', domain_accuracy, step=epoch)\n",
        "\n",
        "    print('Train: Epoch %s: Seg Loss: %.4f, Seg Acc: %.4f, Dom Loss: %.4f, Dom Acc: %.4f' % \n",
        "        (epoch, segmentation_loss, segmentation_accuracy, domain_loss, domain_accuracy))\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "            (segmentatin_loss,\n",
        "            segmentation_accuracy,\n",
        "            domain_loss,\n",
        "            domain_accuracy) = ignore_case_validation_epoch(model, \n",
        "                                d_model, \n",
        "                                BATCH_SIZE, \n",
        "                                val_gen, \n",
        "                                tar_gen,\n",
        "                                weighted_crossentropy,\n",
        "                                binary_crossentropy,\n",
        "                                accuracy,\n",
        "                                binary_accuracy\n",
        "                                )\n",
        "            with test_summary_writer.as_default():\n",
        "                tf.summary.scalar('test segmentation loss', segmentatin_loss, step=epoch)\n",
        "                tf.summary.scalar('test segmentation accuracy', segmentation_accuracy, step=epoch)\n",
        "                tf.summary.scalar('test domain loss', domain_loss, step=epoch)\n",
        "                tf.summary.scalar('test domain accuracy', domain_accuracy, step=epoch)\n",
        "\n",
        "            print('Test: Epoch %s: Seg Loss: %.4f, Seg Acc: %.4f, Dom Loss: %.4f, Dom Acc: %.4f' % \n",
        "                    (epoch, segmentation_loss, segmentation_accuracy, domain_loss, domain_accuracy))\n",
        "            \n",
        "            model.save_weights(f'/content/drive/MyDrive/weights/DAUNet8/{epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load best weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5WwcRQ2Oj-5",
        "outputId": "e959db96-f496-4527-cef8-0cb2d2af8bce"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/weights/DAUNet8/74\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "H7KL_hy8Oa3E",
        "outputId": "9e83e2b1-03a9-4869-bc88-1a31ae891aa8"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix_for_ignore_case(model, train_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "K8WYVQJyO5xv",
        "outputId": "e83ca8aa-5c38-4470-9f60-95f4612628c4"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix_for_ignore_case(model, val_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "efVoXxviO9gx",
        "outputId": "8ac660f4-6f0c-4529-964a-d819d19104fd"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix_for_ignore_case(model, test_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "y84ajL47PCzo",
        "outputId": "dd365ab2-9b77-429e-92e3-e79eb7047868"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix_for_ignore_case(model, tar_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAbLmvfcPIL0",
        "outputId": "35612228-304e-4874-a79a-f22f1638c68e"
      },
      "outputs": [],
      "source": [
        "meanf1_iou_for_ignore_case(model, test_gen, ignore=4)\n",
        "meanf1_iou_for_ignore_case(model, tar_gen, ignore=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn3zKNjwXu9C"
      },
      "outputs": [],
      "source": [
        "plot_feature_space(model, files, tar_files, minimum, \n",
        "                                maximum, normalize=True, gan=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "EnhancedDAUnet2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
